{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Computing - Summer 2017\n",
    "# Exercise 4 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4.1. K-means Clustering of couse network data\n",
    "\n",
    "Write a Python program that computes K-means clustering for the course dataset. The input dataset (file: networkinput.csv) contains data from our social networking platform. As always this data has been anonymized. Each line in the file represents one feature vector and is associated with a participant in the social networking platform. The vectors have a name (\"userXYZ\") and the following 4 features:\n",
    "* Number of posts\n",
    "* Number of comments\n",
    "* Number of likes (on both posts and comments)\n",
    "* Number of friends and followers (average)\n",
    "\n",
    "By clustering the participants, you identify particpants with similar activity patterns. This can be helpful for research but also for advertisers and polling firms.\n",
    "\n",
    "Your program should compute K-means clustering for the dataset according to the formula discussed in the lecture ==> Minimizing the objective function:\n",
    "$$\\sum_{k=1}^{K} \\sum_{\\{n|x_n \\in C_k \\}} \\|x_n - \\mu_k\\|^2$$\n",
    "Such that:\n",
    "$K$ is the number of clusters, $x_n$ is the nth point that belongs to the $k$th cluster, and $\\mu_k$ is the centroid (prototype) of the $k$th cluster. (Refer to the lecture for details)\n",
    "The K-means clustering algorithm should proceed as follows:\n",
    "1. The program should start by parsing the dataset \n",
    "2. Assign four random centroids (prototypes).\n",
    "3. Assign data points to the nearest centroid. \n",
    "4. Recompute the centroid values: The new centroid values of the kth centroid are calculated as the average values of the points currently in that centroid.\n",
    "5. Repeat from point 3 till the values of the centroids don't change anymore.\n",
    "\n",
    "<b>The output of your program should be a a list that asigns a cluster ID (0, 1, 2, 3) to every user in the input file. </b>\n",
    "The first argument in that tuple should be the users's name (e.g., \"user111\") and the second argument should be the centroid id to which this user is associated to. e.g ('user111', 3).\n",
    "**Note:** this output value should be the final centroid values that don't change anymore.\n",
    "\n",
    "\n",
    "<b> After the clustering with k=4 is complete, run the code with the following centroid starting points:\n",
    "\n",
    "centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}.\n",
    "\n",
    "Have a look at the result and describe the common properties in each of the four groups</b> (max 5 sentences).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: [0.8426966292134831,\n",
       "   7.348314606741573,\n",
       "   6.719101123595506,\n",
       "   9.52808988764045],\n",
       "  1: [0.05813953488372093,\n",
       "   0.7209302325581395,\n",
       "   0.36046511627906974,\n",
       "   2.546511627906977],\n",
       "  2: [2.96, 11.44, 25.56, 13.76],\n",
       "  3: [5.333333333333333, 29.333333333333332, 55.0, 55.333333333333336]},\n",
       " {'user1003': 2,\n",
       "  'user1012': 0,\n",
       "  'user10132': 1,\n",
       "  'user10138': 1,\n",
       "  'user10144': 1,\n",
       "  'user10159': 1,\n",
       "  'user10168': 1,\n",
       "  'user1021': 1,\n",
       "  'user10258': 1,\n",
       "  'user10465': 1,\n",
       "  'user10474': 1,\n",
       "  'user1066': 0,\n",
       "  'user11137': 1,\n",
       "  'user1126': 0,\n",
       "  'user11266': 1,\n",
       "  'user1129': 2,\n",
       "  'user11494': 1,\n",
       "  'user115': 0,\n",
       "  'user11509': 1,\n",
       "  'user11563': 1,\n",
       "  'user11584': 1,\n",
       "  'user11587': 1,\n",
       "  'user11602': 1,\n",
       "  'user11608': 1,\n",
       "  'user11629': 1,\n",
       "  'user11632': 1,\n",
       "  'user11638': 1,\n",
       "  'user11644': 1,\n",
       "  'user11647': 1,\n",
       "  'user11650': 1,\n",
       "  'user11653': 1,\n",
       "  'user11656': 1,\n",
       "  'user11659': 1,\n",
       "  'user11662': 1,\n",
       "  'user11665': 1,\n",
       "  'user11674': 1,\n",
       "  'user11677': 1,\n",
       "  'user11683': 1,\n",
       "  'user11692': 1,\n",
       "  'user11698': 1,\n",
       "  'user11704': 1,\n",
       "  'user11707': 1,\n",
       "  'user11716': 1,\n",
       "  'user11722': 1,\n",
       "  'user11731': 1,\n",
       "  'user11749': 1,\n",
       "  'user11752': 1,\n",
       "  'user11758': 1,\n",
       "  'user1189': 1,\n",
       "  'user1195': 1,\n",
       "  'user11971': 1,\n",
       "  'user1198': 2,\n",
       "  'user12064': 1,\n",
       "  'user12076': 1,\n",
       "  'user12082': 1,\n",
       "  'user12112': 1,\n",
       "  'user12124': 1,\n",
       "  'user12142': 1,\n",
       "  'user1219': 1,\n",
       "  'user1237': 1,\n",
       "  'user1240': 0,\n",
       "  'user1243': 1,\n",
       "  'user12445': 1,\n",
       "  'user1246': 1,\n",
       "  'user12571': 1,\n",
       "  'user12598': 1,\n",
       "  'user12622': 1,\n",
       "  'user12664': 1,\n",
       "  'user12676': 1,\n",
       "  'user142': 0,\n",
       "  'user145': 1,\n",
       "  'user148': 2,\n",
       "  'user1501': 1,\n",
       "  'user151': 0,\n",
       "  'user154': 0,\n",
       "  'user157': 0,\n",
       "  'user160': 1,\n",
       "  'user1609': 0,\n",
       "  'user1612': 1,\n",
       "  'user1615': 1,\n",
       "  'user1618': 1,\n",
       "  'user163': 1,\n",
       "  'user1636': 1,\n",
       "  'user1648': 0,\n",
       "  'user166': 1,\n",
       "  'user1663': 1,\n",
       "  'user169': 2,\n",
       "  'user1702': 0,\n",
       "  'user1705': 2,\n",
       "  'user172': 1,\n",
       "  'user178': 0,\n",
       "  'user181': 2,\n",
       "  'user1810': 0,\n",
       "  'user1834': 2,\n",
       "  'user1837': 0,\n",
       "  'user184': 1,\n",
       "  'user1864': 0,\n",
       "  'user187': 2,\n",
       "  'user190': 0,\n",
       "  'user1906': 2,\n",
       "  'user193': 1,\n",
       "  'user1930': 1,\n",
       "  'user1936': 0,\n",
       "  'user1945': 1,\n",
       "  'user196': 1,\n",
       "  'user199': 2,\n",
       "  'user208': 3,\n",
       "  'user211': 3,\n",
       "  'user214': 2,\n",
       "  'user2296': 0,\n",
       "  'user2305': 1,\n",
       "  'user232': 0,\n",
       "  'user2320': 1,\n",
       "  'user2329': 0,\n",
       "  'user2341': 2,\n",
       "  'user2350': 0,\n",
       "  'user2353': 0,\n",
       "  'user2362': 1,\n",
       "  'user2368': 1,\n",
       "  'user2398': 2,\n",
       "  'user2419': 1,\n",
       "  'user2422': 1,\n",
       "  'user2440': 1,\n",
       "  'user2476': 1,\n",
       "  'user2497': 1,\n",
       "  'user2500': 0,\n",
       "  'user2509': 1,\n",
       "  'user2611': 2,\n",
       "  'user2659': 1,\n",
       "  'user268': 1,\n",
       "  'user2680': 0,\n",
       "  'user2683': 3,\n",
       "  'user2722': 2,\n",
       "  'user2731': 0,\n",
       "  'user2761': 1,\n",
       "  'user2764': 0,\n",
       "  'user2872': 1,\n",
       "  'user2890': 0,\n",
       "  'user2905': 0,\n",
       "  'user2914': 1,\n",
       "  'user2920': 1,\n",
       "  'user2929': 0,\n",
       "  'user2932': 1,\n",
       "  'user2974': 1,\n",
       "  'user307': 2,\n",
       "  'user3124': 1,\n",
       "  'user3148': 1,\n",
       "  'user3169': 0,\n",
       "  'user3184': 2,\n",
       "  'user3220': 1,\n",
       "  'user325': 0,\n",
       "  'user3274': 0,\n",
       "  'user3277': 0,\n",
       "  'user331': 1,\n",
       "  'user334': 1,\n",
       "  'user3367': 0,\n",
       "  'user337': 1,\n",
       "  'user340': 1,\n",
       "  'user343': 1,\n",
       "  'user3448': 1,\n",
       "  'user346': 1,\n",
       "  'user3520': 0,\n",
       "  'user3523': 2,\n",
       "  'user3538': 1,\n",
       "  'user3547': 1,\n",
       "  'user3559': 1,\n",
       "  'user358': 0,\n",
       "  'user361': 1,\n",
       "  'user364': 0,\n",
       "  'user3655': 0,\n",
       "  'user367': 0,\n",
       "  'user3688': 1,\n",
       "  'user3694': 1,\n",
       "  'user3700': 1,\n",
       "  'user3721': 1,\n",
       "  'user373': 2,\n",
       "  'user376': 1,\n",
       "  'user3817': 1,\n",
       "  'user388': 0,\n",
       "  'user3898': 1,\n",
       "  'user3919': 0,\n",
       "  'user394': 0,\n",
       "  'user3961': 0,\n",
       "  'user3967': 0,\n",
       "  'user397': 1,\n",
       "  'user3988': 0,\n",
       "  'user3991': 2,\n",
       "  'user4006': 1,\n",
       "  'user409': 0,\n",
       "  'user412': 0,\n",
       "  'user4132': 0,\n",
       "  'user4144': 1,\n",
       "  'user4159': 0,\n",
       "  'user4243': 0,\n",
       "  'user427': 0,\n",
       "  'user4297': 1,\n",
       "  'user430': 0,\n",
       "  'user4351': 1,\n",
       "  'user436': 0,\n",
       "  'user4360': 1,\n",
       "  'user4363': 1,\n",
       "  'user439': 2,\n",
       "  'user4438': 1,\n",
       "  'user445': 0,\n",
       "  'user457': 1,\n",
       "  'user4612': 0,\n",
       "  'user4657': 1,\n",
       "  'user466': 0,\n",
       "  'user4660': 1,\n",
       "  'user4666': 1,\n",
       "  'user4696': 1,\n",
       "  'user4708': 1,\n",
       "  'user4723': 1,\n",
       "  'user4726': 1,\n",
       "  'user4738': 1,\n",
       "  'user4759': 1,\n",
       "  'user4768': 0,\n",
       "  'user4771': 1,\n",
       "  'user4795': 1,\n",
       "  'user4816': 1,\n",
       "  'user4825': 1,\n",
       "  'user484': 1,\n",
       "  'user4852': 0,\n",
       "  'user4867': 1,\n",
       "  'user4876': 1,\n",
       "  'user4903': 1,\n",
       "  'user4927': 1,\n",
       "  'user493': 0,\n",
       "  'user5011': 0,\n",
       "  'user5017': 1,\n",
       "  'user508': 0,\n",
       "  'user5113': 2,\n",
       "  'user5137': 1,\n",
       "  'user5140': 1,\n",
       "  'user5152': 1,\n",
       "  'user5155': 0,\n",
       "  'user5173': 1,\n",
       "  'user5176': 1,\n",
       "  'user5206': 0,\n",
       "  'user5212': 1,\n",
       "  'user5266': 1,\n",
       "  'user5374': 1,\n",
       "  'user5392': 1,\n",
       "  'user5542': 1,\n",
       "  'user562': 1,\n",
       "  'user5620': 0,\n",
       "  'user565': 1,\n",
       "  'user5650': 1,\n",
       "  'user5677': 1,\n",
       "  'user5686': 1,\n",
       "  'user574': 0,\n",
       "  'user5800': 1,\n",
       "  'user5836': 0,\n",
       "  'user5863': 1,\n",
       "  'user5896': 1,\n",
       "  'user5908': 1,\n",
       "  'user5911': 1,\n",
       "  'user5914': 1,\n",
       "  'user5947': 1,\n",
       "  'user5995': 1,\n",
       "  'user6127': 1,\n",
       "  'user6136': 1,\n",
       "  'user6166': 1,\n",
       "  'user6169': 0,\n",
       "  'user6175': 1,\n",
       "  'user6184': 1,\n",
       "  'user6187': 1,\n",
       "  'user6199': 0,\n",
       "  'user6202': 0,\n",
       "  'user6259': 0,\n",
       "  'user6265': 1,\n",
       "  'user6289': 1,\n",
       "  'user6310': 1,\n",
       "  'user6328': 1,\n",
       "  'user6382': 1,\n",
       "  'user6421': 1,\n",
       "  'user6460': 1,\n",
       "  'user6469': 1,\n",
       "  'user649': 0,\n",
       "  'user6502': 1,\n",
       "  'user655': 0,\n",
       "  'user6568': 1,\n",
       "  'user658': 1,\n",
       "  'user6655': 1,\n",
       "  'user6658': 1,\n",
       "  'user6745': 1,\n",
       "  'user6868': 1,\n",
       "  'user688': 0,\n",
       "  'user691': 2,\n",
       "  'user6931': 1,\n",
       "  'user6979': 1,\n",
       "  'user7102': 1,\n",
       "  'user7186': 1,\n",
       "  'user7222': 1,\n",
       "  'user7375': 1,\n",
       "  'user7384': 1,\n",
       "  'user7417': 1,\n",
       "  'user742': 1,\n",
       "  'user7444': 1,\n",
       "  'user7555': 1,\n",
       "  'user7600': 1,\n",
       "  'user7603': 1,\n",
       "  'user7615': 1,\n",
       "  'user7636': 1,\n",
       "  'user7642': 1,\n",
       "  'user7645': 1,\n",
       "  'user7663': 1,\n",
       "  'user7675': 1,\n",
       "  'user7684': 1,\n",
       "  'user7693': 1,\n",
       "  'user7708': 1,\n",
       "  'user7714': 1,\n",
       "  'user7717': 1,\n",
       "  'user775': 1,\n",
       "  'user7762': 1,\n",
       "  'user778': 1,\n",
       "  'user781': 0,\n",
       "  'user784': 0,\n",
       "  'user7855': 1,\n",
       "  'user7873': 0,\n",
       "  'user7891': 1,\n",
       "  'user7894': 1,\n",
       "  'user7957': 1,\n",
       "  'user8011': 2,\n",
       "  'user8098': 1,\n",
       "  'user8116': 1,\n",
       "  'user8131': 0,\n",
       "  'user814': 0,\n",
       "  'user817': 0,\n",
       "  'user8173': 1,\n",
       "  'user820': 1,\n",
       "  'user8227': 1,\n",
       "  'user829': 1,\n",
       "  'user8308': 1,\n",
       "  'user832': 0,\n",
       "  'user8353': 1,\n",
       "  'user8392': 0,\n",
       "  'user847': 0,\n",
       "  'user8494': 1,\n",
       "  'user856': 0,\n",
       "  'user877': 0,\n",
       "  'user8779': 1,\n",
       "  'user8824': 1,\n",
       "  'user8884': 1,\n",
       "  'user8920': 1,\n",
       "  'user9079': 1,\n",
       "  'user9082': 1,\n",
       "  'user9112': 1,\n",
       "  'user9115': 1,\n",
       "  'user9121': 1,\n",
       "  'user9172': 1,\n",
       "  'user9196': 1,\n",
       "  'user9202': 0,\n",
       "  'user9217': 1,\n",
       "  'user9274': 1,\n",
       "  'user928': 0,\n",
       "  'user931': 1,\n",
       "  'user9367': 1,\n",
       "  'user9394': 1,\n",
       "  'user943': 0,\n",
       "  'user9454': 1,\n",
       "  'user9466': 1,\n",
       "  'user9469': 1,\n",
       "  'user9472': 1,\n",
       "  'user9493': 1,\n",
       "  'user9529': 1,\n",
       "  'user9595': 1,\n",
       "  'user9628': 1,\n",
       "  'user964': 0,\n",
       "  'user9793': 1,\n",
       "  'user9802': 1,\n",
       "  'user9820': 1,\n",
       "  'user9826': 1,\n",
       "  'user9838': 1,\n",
       "  'user9928': 1})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv \n",
    "import random\n",
    "import numpy as np \n",
    "from scipy.spatial import distance \n",
    "from collections import OrderedDict\n",
    "from __future__ import division\n",
    "\n",
    "# TODO: Read the network activity data set into a dictionary (observations_dictionary)\n",
    "def read_data_set(data_set_path):\n",
    "    reader = csv.reader(open(data_set_path))\n",
    "    observations_dictionary = OrderedDict()\n",
    "    for row in reader:\n",
    "        key = row[0]\n",
    "        observations_dictionary[key] = row[1:]\n",
    "        for i in observations_dictionary.keys():\n",
    "            observations_dictionary[i] = [int(j) for j in observations_dictionary[i]]\n",
    "    return observations_dictionary\n",
    "\n",
    "# TODO: Assign random centroids\n",
    "# Hint: centroid values should be randomly assigned so that for each dimension of the centroid, the random values should fall\n",
    "# between the maximum and the minimum value of that dimension of all the points in the data set.\n",
    "# For example: for the dimension \"Number of Comments\", if the maximum value = 49, and the minimum value = 0, the value assigned to\n",
    "# the dimension \"Murder\" of the centroid should be randomly assigned between 0 and 49\n",
    "def create_random_centroid_values(observations, centroid,k):\n",
    "    # find min and max of observations in each dimension\n",
    "    number_of_posts, number_of_comments, number_of_likes, number_of_friends_and_followers = [],[],[],[]\n",
    "    for row in observations.values():\n",
    "        number_of_posts.append(int(row[0]))\n",
    "        number_of_comments.append(int(row[1]))\n",
    "        number_of_likes.append(int(row[2]))\n",
    "        number_of_friends_and_followers.append(int(row[3]))\n",
    "    min_posts = min(number_of_posts)\n",
    "    max_posts = max(number_of_posts)\n",
    "    min_comments = min(number_of_comments)\n",
    "    max_comments = max(number_of_comments)\n",
    "    min_likes = min(number_of_likes)\n",
    "    max_likes = max(number_of_likes)\n",
    "    min_friends = min(number_of_friends_and_followers)\n",
    "    max_friends = max(number_of_friends_and_followers)\n",
    "    # create dictionary with four random centroids within the observed space\n",
    "    for i in range(0,k):\n",
    "        centroid_list =[]\n",
    "        centroid_list.append(random.randint(min_posts,max_posts))\n",
    "        centroid_list.append(random.randint(min_comments,max_comments))\n",
    "        centroid_list.append(random.randint(min_likes,max_likes))\n",
    "        centroid_list.append(random.randint(min_friends,max_friends)) \n",
    "        centroid[i] = centroid_list\n",
    "    return centroid  \n",
    "\n",
    "# create_random_centroid_values(read_data_set('networkinput.csv'),{},4)\n",
    "\n",
    "# Assign centroid ID for each of the data points. Each data item is assigned to its closest centroid\n",
    "def update_observation_centroids(observations, centroids, k, observation_centroids = None):\n",
    "    # create dictionary mapping each observation to a centroid index\n",
    "    # initial run: For each centroid i of the k centroids, create random values\n",
    "    observation_centroids = {}\n",
    "    if centroids == None:\n",
    "        centroids = create_random_centroid_values(observations, {},k)\n",
    "                 \n",
    "    # TODO: Each centroid i of the K centroids is the average of the data values previously assigned to that centroid i                \n",
    "    for key in observations:\n",
    "        u_id_values = observations[key]\n",
    "        center = centroids[0]\n",
    "        observation_centroids[key] = 0\n",
    "        smallest_dist = distance.euclidean(u_id_values, centroids[0])\n",
    "        for i in range(1,k): \n",
    "            dist = distance.euclidean(u_id_values, centroids[i])\n",
    "            if(dist<smallest_dist):\n",
    "                smallest_dist = dist        \n",
    "                observation_centroids[key] = i\n",
    "            # TODO: Return the newly created observation centroids\n",
    "    return observation_centroids\n",
    "\n",
    "# update_observation_centroids(read_data_set('networkinput.csv'), None, 4, observation_centroids = None)\n",
    "\n",
    "\n",
    "# Assign centroid ID for each of the data points. Each data item is assigned to its closest centroid\n",
    "def update_observation_centroids(observations, centroids, k, observation_centroids = None):\n",
    "# create dictionary mapping each observation to a centroid index\n",
    "    observation_centroids = {}\n",
    "    # initial run: For each centroid i of the k centroids, create random values\n",
    "    for key in observations:\n",
    "        x = observations[key]\n",
    "        c = centroids[0]\n",
    "        observation_centroids[key] = 0\n",
    "        best_fit = distance.euclidean(np.array(x),np.array(c))\n",
    "        for i in range(1,k):\n",
    "            c = centroids[i]\n",
    "            distance_i = distance.euclidean(np.array(x),np.array(c))\n",
    "            if(distance_i<best_fit):\n",
    "                observation_centroids[key] = i\n",
    "                best_fit = distance_i       \n",
    "    # TODO: return the updated centroids\n",
    "    return observation_centroids\n",
    "\n",
    "\n",
    "# Create new centroid values for each cluster as the mean of data values for the points in that cluster \n",
    "def update_centroid_values(observations, observation_centroids, k):\n",
    "    c_set = {0:[],1:[],2:[],3:[]}\n",
    "    new_centroids = {}\n",
    "    for u_id, c_id in observation_centroids.iteritems():\n",
    "        c_set.setdefault(c_id,[]).append(u_id)\n",
    "    for i in range(0,k):\n",
    "        set_length = len(c_set[i])\n",
    "        sum_of_posts = 0\n",
    "        sum_of_comments = 0\n",
    "        sum_of_likes = 0\n",
    "        sum_of_friends = 0\n",
    "        for u_id in range(0,set_length):\n",
    "            #sum of posts is the posts corresponding to the user_id \n",
    "            sum_of_posts = sum_of_posts + observations[c_set[i][u_id]][0] \n",
    "            sum_of_comments = sum_of_comments + observations[c_set[i][u_id]][1]\n",
    "            sum_of_likes = sum_of_likes + observations[c_set[i][u_id]][2]\n",
    "            sum_of_friends = sum_of_friends + observations[c_set[i][u_id]][3]\n",
    "        if(set_length == 0):\n",
    "            set_length = 1        \n",
    "        new_centroids[i] = [(sum_of_posts*1.0)/set_length, (sum_of_comments*1.0)/set_length, (sum_of_likes*1.0)/set_length, (sum_of_friends*1.0)/set_length]\n",
    "    return new_centroids   \n",
    "\n",
    "            \n",
    "def calculate_k_means_clustering(data_set_path, k):\n",
    "    observations = read_data_set(data_set_path)\n",
    "#     centroids = update_observation_centroids(observations, k)\n",
    "    centroids = None\n",
    "    new_centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}\n",
    "    observation_centroids = None\n",
    "    # TODO: Compare the new centroid dictionary with the old centroid dictionary\n",
    "    while (new_centroids != centroids):# new and old centroid dictionaries are NOT similar, do:\n",
    "        centroids = new_centroids\n",
    "        observation_centroids = update_observation_centroids(observations, centroids, k, observation_centroids)\n",
    "        new_centroids = update_centroid_values(observations, observation_centroids, k)        \n",
    "    return new_centroids,observation_centroids\n",
    "        \n",
    "        \n",
    "        \n",
    "# run code\n",
    "calculate_k_means_clustering('networkinput.csv', 4)\n",
    "\n",
    "# # print update_centroid_values(observations, 4,{0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the clustering with k=4 is complete, with the centroid starting points:\n",
    "centroids = {0: [9, 33, 29, 25], 1: [4, 44, 12, 41], 2: [10, 13, 44, 65], 3: [10, 44, 48, 70]}, the K means algorithm took 10 rounds to converge.The centroids are the following:\n",
    " {0: [0.8426966292134831, 7.348314606741573, 6.719101123595506, 9.52808988764045], 1: [0.05813953488372093,  0.7209302325581395, 0.36046511627906974, 2.546511627906977], 2: [2.96, 11.44, 25.56, 13.76],\n",
    "  3: [5.333333333333333, 29.333333333333332, 55.0, 55.333333333333336]}\n",
    " \n",
    "89 users in cluster_0, 258 users in cluster_1, 25 users in cluster_2 and 3 users in cluster_4.\n",
    "Most of the users are in cluster 1 near the centroid [0.05813953488372093,  0.7209302325581395, 0.36046511627906974, 2.546511627906977] which are the lowest centroid values.\n",
    "\n",
    "In all the 4 clusters, the dimension for the number of posts is the minimum as compared to other dimensions.\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Problem 4.2. Girvan-Newman Algorithm \n",
    "The Girvan-Newman algorithm is an efficient algorithm for computing graph clustering.\n",
    "\n",
    "Write a Python program that computes the best clustering for the Krackhardt Kite graph.\n",
    "Remember from the lecture that the central idea of the Girvan-Newman algorithm is to compute the edge betweenness in the input graph. Large betweenness value is an indication that the corresponding edge is a bridge between two clusters in the graph, and cutting that edge means isolating those clusters.\n",
    "The algorithm proceeds by determining edge betweenness values for all the edges in the graph, and removing the edge with the highest betweenness value and repeating until there are no more edges.\n",
    "The output of the Girvan-Newman algorithm is a dendrogram of clusters where individual vertices are at the bottom. Therefore, it's necessary to cut that dendrogram and determine the best cluster. Best clustering is the one with the highest graph modularity value, which is determined by the formula:\n",
    "$$ Q = \\sum_{i} (e_{ii} - a_i^2) $$\n",
    "where $e_{ii}$ sums the fraction of graph edges that connects nodes in the ith cluster. And $a_i$ is the fraction of edges that connect to the ith cluster (see lecture for details)\n",
    "The input to the Python program will be the Krackhardt Kite Graph as an igraph Graph object. The output should be a tuple of two arguments:\n",
    "The first should be the value of the best modularity corresponding to the best graph clustering (a floating point number). The second argument should be a tuple of igraph Graph objects representing the clusters. \n",
    "HINT: The edge betweenness calculation is a variation of the Ulrik Brandes algorithm for calculating betweenness centrality (exercise 1). The attached paper explains how to modify the algorithm to calculate edge betweenness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import igraph\n",
    "# import Queue\n",
    "# from __future__ import division\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def calculate_node_degrees(g):\n",
    "#     # HINT: You will need to calculate the adjacency matrix of the graph\n",
    "#     # Some Graph methods from igraph could be helpful\n",
    "\n",
    "# def calculate_modularity(g, original_deg_dict, m):\n",
    "#     modularity = 0\n",
    "#     degree_dict = calculate_node_degrees(g)\n",
    "#     connected_components = g.components()\n",
    "#     for i in range(len(connected_components)):\n",
    "#         subgraph = connected_components.subgraph(i) # each subgraph represents a cluster in the graph\n",
    "#         e = 0 # Fraction of edges that connect to cluster\n",
    "#         a = 0 # Fraction of edges that connect to cluster with random connections between edges\n",
    "#         for v in subgraph.vs:\n",
    "#             e += degree_dict[v.index]\n",
    "#             a += original_deg_dict[v.index]\n",
    "#         # TODO: Calculate the modularity\n",
    "#     return modularity\n",
    "        \n",
    "# # TODO: Calculate edge betweeness  \n",
    "# def calculate_edge_betweenness(g):\n",
    "\n",
    "# def calculate_girvan_newman_clustering(g):\n",
    "#     m = # TODO: The original number of edges\n",
    "#     original_degree = calculate_node_degrees(g)\n",
    "#     while # TODO: Graph still has edges:\n",
    "#         Q = calculate_modularity(g, original_degree, m)\n",
    "#         if (Q > largest_modularity):\n",
    "#             largest_modulatirty = Q\n",
    "#             edge_betweenness = calculate_edge_betweenness(g) # TODO: get edge with the highest betweenness value\n",
    "#             # TODO: delete the edge with the maximum betweenness value\n",
    "#             # Hint: Some built-in methods from igraph could be helpful\n",
    "    \n",
    "#     # Finally\n",
    "#     return final_graph_clustering, largest_modularity\n",
    "\n",
    "# # Program's entry point:\n",
    "# g = igraph.Graph.Famous('Krackhardt_Kite') # Connected, Unweighted, undirected social network\n",
    "# calculate_girvan_newman_clustering(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
